4/22
____

Started project. As we mentioned in our proposal, since we are just a group of two,
we aren't sure how much of the entire BitTorrent protocol we will be able to fully
implement. Thus, we started working on a prototype that would have the most basic 
structure of the general BitTorrent architecture. 

Wrote the b-encoding helper functions and reused some chunks of code from previous 
assignments to get the communication over-the-wire working. We added functionality 
for peers and the tracker to send each other messages that represent either a 
"first-contact" handshake to tell each other what pieces of the file the possess
and things like their peer ids, or connections to send over a requested piece of
the overall file.

4/23
____

Since the whole BitTorrent architecture would be difficult to set up in its entirety,
and because we want to have a good amount of progress made (and at least some kind of
demo-able prdouct by the Design Fair), we decided to scale back our idea for our 
prototype to have some much simpler features that would make it easier to quickly 
implement. 

Our current methodology simply uses files stored in folders to represent each peer's 
current pieces of the overall file, and downloading from one another is represented 
by peers writing files from bytes passed along the wire by other peers into new files 
in their respective folders. We start each client in the director of a given folder 
(peer1, peer2, peer3, etc.), and we can change the initial conditions what the files 
are and which peer starts with which pieces of the file. 

4/25
____

Added a timing feature and a script to generate larger files so we can see how long
downloads actually take. From some baseline testing across several trials, it takes 
an average of approximately 10.194 seconds for each of five clients to download all 
the pieces of a file totalling a gigabyte. This, of course, would not reflect the 
real time it would take should this be implemented over the Internet, as we are 
testing this locally on a singular laptop, but this serves as a point of comparison
for our other tests. There is also variation in the time across clients 
because we sometimes start some clients with more pieces than others initially. 

A design choice we made here was to start with at least one copy of each pieces of 
the file have already in circulation among the peers before running our simulation,
as opposed to having a central location where peers might first have to download 
from initially before they can start to seed the files (which would probably be more
reflective of torrenting in the real world). We made this choice chiefly because it 
doesn't really add much to the theoretical component of our implementation, and it 
would also be relatively equivalent to just adding an extra starting peer that just 
has the initial files while starting the rest of the peers with zero files. Testing 
this scenario yields a similar time average of 11.144 seconds. It makes sense that 
this takes longer than the first scenario, since the pieces start with a peer that 
others have to download from first before they can start to seed—the initial testing
started each peer with at least one file, so they could all start seeding. To worsen
the problem, the current implementation has peers that download the pieces in order
starting at index 0, as we have not yet implemented a rarest-first piece selection
algorithm.

After implementing the rarest-first piece selection algorithm, we found that the new
average download time has decreased to approximately 7.288 seconds, which is a clear
improvement over our initial simulations. For the scenario where we start with only
one peer that has all the files, we also see significant improvement, now taking an 
average of 9.408 seconds. 

The reason that the rarest-first algorithm has such a large effect is because this 
allows a much greater degree of parallelization, which is one of the largest benefits 
of a BitTorrent network in the first place. The original algorithm was rather 
inefficient because the peers would try to all download piece 0 from peer1. By having
peers target the rarest pieces in the general network, this allows this piece to 
begin being seeded, which gives more peers the ability to download these rarer files
and/or upload their own files. In addition, if there is a tie for the rarest piece, 
a random rarest piece is chosen, and a random peer who possesses that piece is picked
to be requested from. This randomness is introduced to prevent all of the clients 
requesting a given piece from the same peer, which happened a lot initially because
we set the peers to check for possible peers to download from in increasing index
order. 

4/26
____

Created poster for design fair (added to repo) and tests for functions in utils.py, 
peer.py, and tracker.py.

5/4
____
Tried to use the last day to implement the Kademlia DHT protocol.

Kademlia assigns each node and each data key a 160-bit identifier, and arranges known 
peers into “k-buckets” based on the XOR distance between IDs. To locate a key or 
node, Kademlia performs an iterative lookup: it selects the α closest contacts it 
knows, sends parallel RPCs to them, and continues with any newly discovered, closer 
nodes. The four core RPCs—PING (liveness), STORE (replicate a key–value pair), 
FIND_NODE (discover peers), and FIND_VALUE (retrieve data or nearest nodes)—drive all 
lookups and maintenance. When a node joins, it “bootstraps” by querying one or more 
well-known peers for their closest contacts to its own ID, then runs a full iterative 
lookup on itself to fill its routing table. Buckets split only when they’re full and 
contain the node’s own ID-range, and least-recently-seen peers are evicted first, 
keeping the routing table fresh and ensuring robust, efficient searches.

Could not get all parts to work, but we have a draft of this DHT protocol that can be 
seen in DHT.py. In particular, we discovered that it's actually quite hard to get the 
nodes in the network to find each other, and so every node would just assume it was 
the first node in the network and start off its own hash table. With more time, we 
would be able to refactor the code to ensure proper finding of other nodes in the 
network. However, since we are coming up on the deadline for this project, we decided 
to ditch this attempt. This isn't too disappointing, since the trackerless version of 
BitTorrent using a distributed hash table was only introduced later on and so this 
was never a "required" part of the project for us to complete.